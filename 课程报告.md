<h1 align = "center">机器学习课程报告</h1>

> 



## 1.任务描述

- 自行实现KNN（k-nearest neighbor）算法

- 使用KNN算法分类葡萄酒质量数据集（链接https://www.kaggle.com/shelvigarg/wine-quality-dataset/ ）



## 2.数据集描述

- 使用Wine Quality Dataset数据集，通过从物理化学特性进行数据挖掘来建模葡萄酒质量
- 该数据集包含了13列数据，记录了不同类型的葡萄酒样本的各种特性，包含以下13个字段：type 葡萄酒类型，fixed acidity 固定酸度，volatile acidity 挥发性酸度，citric acid 柠檬酸，residual sugar 残糖，chlorides 氯化物，free sulfur dioxide 游离二氧化硫，total sulfur dioxide 总二氧化硫，density 密度，pH pH值，sulphates 硫酸盐，alcohol 酒精度，quality 质量0 到 10 之间的得分
- 数据集统计白葡萄酒4898项，红葡萄酒1599项，排除含空值的数据后共6463项



## 3.方法介绍

### 3.1数据预处理

首先加载数据集，删除含空行的部分，排除后续计算过程中不完整数据的干扰。
规定输入条件包含12个葡萄酒的物化性质数值，输出结果为葡萄酒质量，经观察发现葡萄酒质量数值仅覆盖3-9范围，故将其分类为3-4 低质量，5-6中等质量，7-9高质量。

```python
def LoadFrame(path):
    # 加载数据
    df = pd.read_csv(path)
    # 删除空值行
    df.dropna(inplace=True)
    # 将酒种类特征字符串转为整数 根据酒质量等级设定分类标签
    df['type'] = df['type'].apply(lambda x: 1 if x == 'white' else 0)
    df['quality'] = df['quality'].apply(lambda x: 1 if x <= 4 else (2 if x <= 6 else 3))

    X = np.array(df[df.columns[0:12]])
    Y = np.array(df.quality)
    return X, Y
```

调用`train_test_split()`数据集打乱并划分，本实验中按照训3：1的比例划分训练集与测试集。

此处可以使用两种方法进行归一化处理：
MinMaxScaler：将数据缩放到[0, 1]范围内，为线性归一化，保持了原始数据的线性关系。StandardScaler：将数据转换为均值为0，标准差为1的分布，将数据转换为标准正态分布。经测试后差别不大，代码中均有保留，可供切换选择

```python
def Normalisation(X, Y):
    # 数据打乱并划分训练集与测试集（3:1）
    X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)
    # 归一化处理
    # 最小-最大归一化方法
    X_train = scalar.fit_transform(X_train_raw)
    X_test = scalar.transform(X_test_raw)
    # Z-score标准化
    # scaler = StandardScaler().fit(X_train_raw)
    # X_train = scaler.transform(X_train_raw)
    # X_test = scaler.transform(X_test_raw)
    return X_train, X_test, Y_train, Y_test
```



### 3.2算法描述

**算法原理**

K 最近邻(k-Nearest Neighbor，KNN)分类算法思路是：在特征空间中，如果一个样本附近的 k 个最近（即特征空间中最邻近）样本的大多数属于某一个类别，则该样本也属于这个类别。

假设已经存在了一个带标签的数据库，然后输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似（最近邻）的分类标签。一般来说，选择样本数据库中前 k 个最相似的数据。最后，选择 k 个最相似数据中出现次数最多的分类。

如图所示：绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果 K=3，由于红色三
角形所占比例为 2/3，绿色圆将被赋予红色三角形那个类，如果 K=5，由于蓝色四方形比例
为 3/5，因此绿色圆被赋予蓝色四方形类。

<img src=".\img1" alt="image-20230917230344202" style="zoom:50%;" />

**算法实现**

算法的具体实现分为如下步骤： 

1. 初始化KNN分类器，设定 k 值加载训练集数据
2. 计算已知类别数据集中的点与当前点之间的距离，本实现中支持选择欧氏距离（详见函数`EuclideanDistance()`）或曼哈顿距离（详见函数`ManhattanDistance()`）的计算
3. 按照距离递增次序排序，选取与当前点距离最小的 k 个点
4. 确定前 k 个点所在类别的出现次数，取前 k 个点出现频率最高的类别作为当前点的预测分类

经过类封装后的代码实现如下：

```python
class KNN:
    def __init__(self, k: int):
        self.k: int = k
        self.xtrain = None
        self.ytrain = None

    def LoadData(self, x, y):
        self.xtrain = x
        self.ytrain = y

    def ManhattanDistance(self, xtest):
        distances = []
        for row in range(len(self.xtrain)):
            distance = np.sum(np.abs(self.xtrain[row] - xtest))
            distances.append((self.xtrain[row], self.ytrain[row], distance))
        distances.sort(key=lambda x: x[2])
        # 截取k个邻居
        return distances[:self.k]

    def EuclideanDistance(self, xtest):
        distances = []
        for row in range(len(self.xtrain)):
            distance = np.sqrt(np.sum((self.xtrain[row] - xtest) ** 2))
            distances.append((self.xtrain[row], self.ytrain[row], distance))
        distances.sort(key=lambda x: x[2])
        # 截取k个邻居
        return distances[:self.k]


    def Predict(self, xtest):
        print(len(xtest))
        predictlist = []
        for i in range(len(xtest)):
            # neighbors = self.ManhattanDistance(xtest[i])
            neighbors = self.EuclideanDistance(xtest[i])

            counter = Counter(neighbor[1] for neighbor in neighbors)
            prediction = counter.most_common(1)[0][0]
            predictlist.append(prediction)
        return predictlist
```



## 4.实验结果分析

### 4.1评价指标

分类准确率：衡量了在测试集中正确预测的标签所占的比例

调用`accuracy_score()`函数对比测试集标签数组和预测标签数组得到准确率

```python
accuracy_score(Y_test, predictlist)
```

执行时间：评估自行实现算法运行效率



### 4.2定量评价结果

与`sklearn.neighbors`库中提供的`KNeighborsClassifier`进行对比，分别进行实例化加载相同的测试集和训练集，实例化分类器核心代码如下

```python
X, Y = LoadFrame("../winequalityN.csv")
X_train, X_test, Y_train, Y_test = Normalisation(X, Y)
knn = KNN(k=3)
knn.LoadData(X_train, Y_train)
predictlist = knn.Predict(X_test)
accuracy1 = accuracy_score(Y_test, predictlist)


clf = KNeighborsClassifier(3)
clf.fit(X_train, Y_train)
Y_pred = clf.predict(X_test)
accuracy2 = accuracy_score(Y_test, Y_pred)
```

以k=3为例，测试对比结果如下：

<img src=".\img2" alt="image-20230917234208856" style="zoom:70%;" />



同一测数据集划分下，对不同k值进行横向对比，测试结果如下表/图（保留两位小数）：

| K    | 自实现KNN算法准确率/耗时 | sklearn库提供KNN算法准确率/耗时 |
| ---- | ------------------------ | ------------------------------- |
| 1    | 82.98% / 39.31           | 82.98% / 0.08                   |
| 2    | 82.98% / 39.90           | 79.33% / 0.09                   |
| 3    | 80.32% / 38.08           | 79.70% / 0.09                   |
| 4    | 82.36% / 39.04           | 79.33% / 0.09                   |
| 5    | 79.82% / 39.10           | 80.81% / 0.10                   |
| 6    | 81.00% / 38.94           | 79.14% / 0.10                   |
| 7    | 79.33% / 38.40           | 79.26% / 0.10                   |
| 8    | 80.63% / 38.86           | 79.45% / 0.12                   |



<img src=".\img3" alt="image-20230918002902488" style="zoom:60%;" />



## 5.总结

- 本项目自行实现了KNN分类算法，并与与现有库中的分类器进行比较。由于只关注算法原理的实现，对于计算过程没有相应优化，存在步骤冗杂、误差积累等问题。

- 总体上准确率基本与现有库中的算法近似，但效率相差较大。

* 在测试过程中，有多种因素会对结果造成影响，例如：数据预处理方式不同、每次划分的测试集与训练集随机、测试集与训练集比例不同、输入特征种类不同等，这些因素都会在一定范围内对结果造成影响，需要合理调整才能达到较好效果。



---

**reference**

https://blog.csdn.net/weixin_46496223/article/details/105646813

https://zhuanlan.zhihu.com/p/600542014
